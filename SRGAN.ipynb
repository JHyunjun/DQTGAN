{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmUZPV1MICsDNf9pB6xUPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/DQTGAN/blob/main/SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6r5QcT5H5jm"
      },
      "outputs": [],
      "source": [
        "# Created by Hunjun, JANG\n",
        "# Recent revision date : 23.07.15\n",
        "# DQT-GAN(Data Quality Transformation-Generative Adversarial Network)\n",
        "\n",
        "!pip install pytube\n",
        "!pip install pydub\n",
        "!pip install librosa\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/GAN/DQT-GAN/Data\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the Path\n",
        "! pwd"
      ],
      "metadata": {
        "id": "HkALOfaCIEqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from pytube import YouTube\n",
        "import tensorflow as tf\n",
        "from pydub import AudioSegment\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "input_sampling_rate = 400\n",
        "output_sampling_rate = 1600\n",
        "clip_duration = 4  # Clip duration in seconds\n",
        "\n",
        "# Define the SRGAN model\n",
        "def srgan_model():\n",
        "    # Generator Model\n",
        "    generator_input = layers.Input(shape=(input_sampling_rate * clip_duration, 1))\n",
        "    x = layers.Conv1D(256, kernel_size=5, padding='same')(generator_input)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(128, kernel_size=5, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(64, kernel_size=5, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(32, kernel_size=5, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.UpSampling1D(int(output_sampling_rate /input_sampling_rate))(x)\n",
        "    generator_output = layers.Conv1D(1, kernel_size=5, padding='same')(x)\n",
        "    generator_model = tf.keras.Model(generator_input, generator_output, name='Generator')\n",
        "\n",
        "    # Discriminator Model\n",
        "    discriminator_input = layers.Input(shape=(output_sampling_rate * clip_duration, 1))\n",
        "    x = layers.Conv1D(32, kernel_size=3, strides=2, padding='same')(discriminator_input)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(64, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv1D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    discriminator_output = layers.Dense(1)(x)\n",
        "    discriminator_model = tf.keras.Model(discriminator_input, discriminator_output, name='Discriminator')\n",
        "\n",
        "    return generator_model, discriminator_model\n",
        "\n",
        "# Define the loss and compile the models\n",
        "def compile_srgan(generator, discriminator):\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-7)\n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-7)\n",
        "    return generator_optimizer, discriminator_optimizer\n",
        "\n",
        "# Download and preprocess the audio\n",
        "def download_and_preprocess(link):\n",
        "    # Download audio\n",
        "    youtube = YouTube(link)\n",
        "    video = youtube.streams.filter(only_audio=True).first()\n",
        "    video.download(filename='audio.mp4')\n",
        "\n",
        "    audio = AudioSegment.from_file('audio.mp4')\n",
        "    audio.export('audio.wav', format='wav')\n",
        "\n",
        "    # Load and resample audio\n",
        "    audio, sr = librosa.load('audio.wav', sr=None, offset=2 * 60, duration = 5 * 60)\n",
        "    audio_8k = librosa.resample(audio, orig_sr=sr, target_sr=input_sampling_rate)\n",
        "    audio_44k = librosa.resample(audio, orig_sr=sr, target_sr=output_sampling_rate)\n",
        "\n",
        "    # Calculate the total number of clips\n",
        "    total_clips = len(audio_8k) - input_sampling_rate * clip_duration + 1\n",
        "\n",
        "    # Slice into n-second clips with overlap\n",
        "    audio_8k_clips = np.array([audio_8k[i:i + input_sampling_rate * clip_duration] for i in range(0, total_clips, input_sampling_rate)])\n",
        "    audio_44k_clips = np.array([audio_44k[i:i + output_sampling_rate * clip_duration] for i in range(0, total_clips, input_sampling_rate)])\n",
        "\n",
        "    return audio_8k_clips, audio_44k_clips\n",
        "\n",
        "\n",
        "# Define a training step\n",
        "def train_step(generator, discriminator, generator_optimizer, discriminator_optimizer, input_audio, target_audio):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_audio = generator(np.expand_dims(input_audio, axis=0), training=True)\n",
        "        real_output = discriminator(np.expand_dims(target_audio, axis=0), training=True)\n",
        "        fake_output = discriminator(generated_audio, training=True)\n",
        "\n",
        "        gen_loss = -tf.reduce_mean(fake_output)\n",
        "        disc_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# Define the training loop\n",
        "def train_srgan(link, epochs):\n",
        "    # Download and preprocess audio\n",
        "    audio_8k, audio_44k = download_and_preprocess(link)\n",
        "\n",
        "    # Build and compile the SRGAN model\n",
        "    generator, discriminator = srgan_model()\n",
        "    generator_optimizer, discriminator_optimizer = compile_srgan(generator, discriminator)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(audio_8k)):\n",
        "            gen_loss, disc_loss = train_step(generator, discriminator, generator_optimizer, discriminator_optimizer, audio_8k[i], audio_44k[i])\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Clip {i+1}/{len(audio_8k)}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}')\n",
        "\n",
        "# Finally, call the training function with the YouTube link and number of epochs\n",
        "train_srgan('https://www.youtube.com/watch?v=83EzIW3MbAI', 5)\n"
      ],
      "metadata": {
        "id": "KyBGPnOtIGIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the validation function for a single sample\n",
        "def validate_sample(generator, input_audio, target_audio, clip_duration):\n",
        "    generated_audio = generator.predict(np.expand_dims(input_audio, axis=0))\n",
        "    generated_audio = generated_audio.squeeze()\n",
        "\n",
        "    # Time axes\n",
        "    time_input = np.arange(input_audio.shape[0]) / input_sampling_rate\n",
        "    time_generated = np.arange(generated_audio.shape[0]) / output_sampling_rate * (clip_duration)\n",
        "    time_target = np.arange(target_audio.shape[0]) / output_sampling_rate * (clip_duration)\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(input_audio, label='Input')\n",
        "    plt.title('Input 0.4kHz')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.xlim(0, input_sampling_rate * clip_duration)\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(generated_audio, label='Generated 1.6kHz')\n",
        "    plt.title('Generated Audio (1.6kHz)')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    #plt.xlim(0, output_sampling_rate * clip_duration)\n",
        "    plt.xlim(0, 20)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(target_audio, label='Target')\n",
        "    plt.title('Original 1.6kHz(Target)')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    #plt.xlim(0, output_sampling_rate * clip_duration)\n",
        "    plt.xlim(0, 20)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Download and preprocess audio\n",
        "audio_8k_clips, audio_44k_clips = download_and_preprocess('https://www.youtube.com/watch?v=83EzIW3MbAI')\n",
        "\n",
        "# Finally, call the validation function with the generator, input audio, and target audio\n",
        "validate_sample(generator, audio_8k_clips[0], audio_44k_clips[0], clip_duration)\n"
      ],
      "metadata": {
        "id": "Xd1aYbNRZut7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}