{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrsm7kLdCmnY8klaysVOgJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/DQTGAN/blob/main/WGANGP_VGGLOSS_CIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUWGImhDY9sG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.models import vgg16\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable, grad\n",
        "\n",
        "# 설정값\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "latent_dim = 100\n",
        "lambda_gp = 10\n",
        "n_critic = 5\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 데이터 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "downsample = transforms.Resize(16)\n",
        "\n",
        "dataset = datasets.CIFAR10(root='./', download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is 3 x 16 x 16\n",
        "            nn.ConvTranspose2d(3, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # Size now is 256 x 32 x 32\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # Size now is 128 x 64 x 64\n",
        "            nn.ConvTranspose2d(128, 3, 3, 1, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # Output size is 3 x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input size is 3 x 64 x 64\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Size now is 128 x 32 x 32\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Size now is 256 x 16 x 16\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # Output is a scalar probability\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)\n",
        "\n",
        "# Pretrained VGG for perceptual loss\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        model = vgg16(pretrained=True)\n",
        "        features = model.features\n",
        "        self.to_relu_1_2 = nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.to_relu_1_2.add_module(str(x), features[x])\n",
        "        self.to_relu_1_2 = self.to_relu_1_2.eval()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = (input + 1) / 2\n",
        "        target = (target + 1) / 2\n",
        "        return torch.nn.functional.l1_loss(self.to_relu_1_2(input), self.to_relu_1_2(target))\n",
        "\n",
        "# WGAN-GP gradient penalty\n",
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = alpha * real + ((1 - alpha) * fake)\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    gradient = grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    penalty = torch.mean((gradient_norm - 1)**2)\n",
        "    return penalty\n",
        "\n",
        "# 모델 생성\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "vgg_loss = VGGPerceptualLoss().to(device)\n",
        "\n",
        "# Optimizers\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    for i, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        real_downsampled = downsample(real).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        D_optimizer.zero_grad()\n",
        "\n",
        "        fake = G(real_downsampled)\n",
        "        real_score = D(real)\n",
        "        fake_score = D(fake)\n",
        "\n",
        "        gp = gradient_penalty(D, real, fake, device)\n",
        "        d_loss = -(torch.mean(real_score) - torch.mean(fake_score)) + lambda_gp * gp\n",
        "\n",
        "        d_loss.backward(retain_graph = True)\n",
        "        D_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        if i % n_critic == 0:\n",
        "            G_optimizer.zero_grad()\n",
        "\n",
        "            fake_score = D(fake)\n",
        "            perceptual_loss = vgg_loss(fake, real)\n",
        "            g_loss = -torch.mean(fake_score) + perceptual_loss\n",
        "\n",
        "            g_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch}/{epochs}] d_loss: {d_loss.item()} g_loss: {g_loss.item()}')\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        utils.save_image(fake.data[:25], f\"{epoch}.png\", nrow=5, normalize=True)\n"
      ],
      "metadata": {
        "id": "10R2PBsebTI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_dataset = datasets.CIFAR10(root='./', train=False, download=True, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 테스트 데이터셋에서 이미지 선택\n",
        "test_images = next(iter(test_dataloader))[0][:25].to(device)  # First batch\n",
        "test_images = downsample(test_images)  # Downsample the test images to 16x16\n",
        "\n",
        "def show_generated_img():\n",
        "    G.eval()  # Set the generator to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        test_images_fake = G(test_images)\n",
        "    G.train()  # Set the generator to training mode\n",
        "\n",
        "    grid = utils.make_grid(test_images_fake, nrow=5, normalize=True).permute(1, 2, 0)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(grid.detach().cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Show generated images\n",
        "show_generated_img()\n"
      ],
      "metadata": {
        "id": "5oAMm_rGkfcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}